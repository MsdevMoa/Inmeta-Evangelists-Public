{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predict NYC Taxi Tips using Spark ML and Azure Data Lake\n",
        "\n",
        "The notebook ingests, visualizes, prepares and then trains a model based on an Open Dataset that tracks NYC Yellow Taxi trips and various attributes around them.\n",
        "The goal is to predict for a given trip whether there will be a trip or not.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql.functions import unix_timestamp\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.ml.feature import RFormula\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ingest DataÂ¶ \n",
        "\n",
        "Get a sample data of nyc yellow taxi to make it faster/easier to evaluate different approaches to prep for the modelling phase later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Primary storage info\n",
        "account_name = ''\n",
        "container_name = ''\n",
        "relative_path = ''\n",
        "\n",
        "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\n",
        "print('Primary storage account path: ' + adls_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Define schema of csv file\n",
        "schema = StructType() \\\n",
        "      .add(\"VendorID\",IntegerType(),True) \\\n",
        "      .add(\"tpep_pickup_datetime\",StringType(),True) \\\n",
        "      .add(\"tpep_dropoff_datetime\",StringType(),True) \\\n",
        "      .add(\"passenger_count\",IntegerType(),True) \\\n",
        "      .add(\"trip_distance\",DoubleType(),True) \\\n",
        "      .add(\"RatecodeID\",StringType(),True) \\\n",
        "      .add(\"store_and_fwd_flag\",StringType(),True) \\\n",
        "      .add(\"PULocationID\",IntegerType(),True) \\\n",
        "      .add(\"DOLocationID\",IntegerType(),True) \\\n",
        "      .add(\"payment_type\",IntegerType(),True) \\\n",
        "      .add(\"fare_amount\",DoubleType(),True) \\\n",
        "      .add(\"extra\",DoubleType(),True) \\\n",
        "      .add(\"mta_tax\",DoubleType(),True) \\\n",
        "      .add(\"tip_amount\",DoubleType(),True) \\\n",
        "      .add(\"tolls_amount\",DoubleType(),True) \\\n",
        "      .add(\"improvement_surcharge\",DoubleType(),True) \\\n",
        "      .add(\"total_amount\",DoubleType(),True) \\\n",
        "\n",
        "# Read data from path into dataframe    \n",
        "nyc_tlc_df = spark.read.format(\"csv\") \\\n",
        "      .option(\"header\", True) \\\n",
        "      .schema(schema) \\\n",
        "      .load(adls_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Display 5 rows\n",
        "nyc_tlc_df.show(5, truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "#To make development easier, faster and less expensive downsample for now\n",
        "sampled_taxi_df = nyc_tlc_df.sample(True, 0.001, seed=1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "#The charting package needs a Pandas dataframe or numpy array do the conversion\n",
        "sampled_taxi_pd_df = sampled_taxi_df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Converting string to datetime\n",
        "sampled_taxi_pd_df['tpep_pickup_datetime'] = pd.to_datetime(sampled_taxi_pd_df['tpep_pickup_datetime'], format='%Y-%m-%d %H:%M:%S')\n",
        "sampled_taxi_pd_df['tpep_dropoff_datetime'] = pd.to_datetime(sampled_taxi_pd_df['tpep_dropoff_datetime'], format='%Y-%m-%d %H:%M:%S')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Print information about dataframe schema\n",
        "print(sampled_taxi_pd_df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis\n",
        "\n",
        "Look at the data and evaluate its suitability for use in a model, do this via some basic charts focussed on tip values and relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Look at tips by amount count histogram\n",
        "ax1 = sampled_taxi_pd_df['tip_amount'].plot(kind='hist', bins=25, facecolor='lightblue')\n",
        "ax1.set_title('Tip amount distribution')\n",
        "ax1.set_xlabel('Tip Amount ($)')\n",
        "ax1.set_ylabel('Counts')\n",
        "plt.suptitle('')\n",
        "plt.show()\n",
        "\n",
        "# How many passengers tip'd by various amounts\n",
        "ax2 = sampled_taxi_pd_df.boxplot(column=['tip_amount'], by=['passenger_count'])\n",
        "ax2.set_title('Tip amount by Passenger count')\n",
        "ax2.set_xlabel('Passenger count') \n",
        "ax2.set_ylabel('Tip Amount ($)')\n",
        "plt.suptitle('')\n",
        "plt.show()\n",
        "\n",
        "# Look at the relationship between fare and tip amounts\n",
        "ax = sampled_taxi_pd_df.plot(kind='scatter', x= 'fare_amount', y = 'tip_amount', c='blue', alpha = 0.10, s=2.5*(sampled_taxi_pd_df['passenger_count']))\n",
        "ax.set_title('Tip amount by Fare amount')\n",
        "ax.set_xlabel('Fare Amount ($)')\n",
        "ax.set_ylabel('Tip Amount ($)')\n",
        "plt.axis([-2, 80, -2, 20])\n",
        "plt.suptitle('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Prep and Featurization\n",
        "\n",
        "It's clear from the visualizations above that there are a bunch of outliers in the data. These will need to be filtered out in addition there are extra variables that are not going to be useful in the model we build at the end.\n",
        "\n",
        "Finally there is a need to create some new (derived) variables that will work better with the model.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {},
      "source": [
        "taxi_df = sampled_taxi_df.select('total_amount', 'fare_amount', 'tip_amount', 'payment_type', 'RatecodeID', 'passenger_count'\\\n",
        "                                , 'trip_distance', 'tpep_pickup_datetime', 'tpep_dropoff_datetime'\\\n",
        "                                , date_format('tpep_pickup_datetime', 'hh').alias('pickupHour')\\\n",
        "                                , date_format('tpep_pickup_datetime', 'EEEE').alias('weekdayString')\\\n",
        "                                , (unix_timestamp(col('tpep_dropoff_datetime')) - unix_timestamp(col('tpep_pickup_datetime'))).alias('tripTimeSecs')\\\n",
        "                                , (when(col('tip_amount') > 0, 1).otherwise(0)).alias('tipped')\n",
        "                                )\\\n",
        "                        .filter((sampled_taxi_df.passenger_count > 0) & (sampled_taxi_df.passenger_count < 8)\\\n",
        "                                & (sampled_taxi_df.tip_amount >= 0) & (sampled_taxi_df.tip_amount <= 25)\\\n",
        "                                & (sampled_taxi_df.fare_amount >= 1) & (sampled_taxi_df.fare_amount <= 250)\\\n",
        "                                & (sampled_taxi_df.tip_amount < sampled_taxi_df.fare_amount)\\\n",
        "                                & (sampled_taxi_df.trip_distance > 0) & (sampled_taxi_df.trip_distance <= 100)\\\n",
        "                                & (sampled_taxi_df.RatecodeID <= 5)\n",
        "                                & (sampled_taxi_df.payment_type.isin({\"1\", \"2\"}))\n",
        "                                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Prep and Featurization Part 2\n",
        "\n",
        "Having created new variables its now possible to drop the columns they were derived from so that the dataframe that goes into the model is the smallest in terms of number of variables, that is required.\n",
        "\n",
        "Also create some more features based on new columns from the first round.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {},
      "source": [
        "taxi_featurised_df = taxi_df.select('total_amount', 'fare_amount', 'tip_amount', 'payment_type', 'passenger_count'\\\n",
        "                                                , 'trip_distance', 'weekdayString', 'pickupHour','tripTimeSecs','tipped'\\\n",
        "                                                , when((taxi_df.pickupHour <= 6) | (taxi_df.pickupHour >= 20),\"Night\")\\\n",
        "                                                .when((taxi_df.pickupHour >= 7) & (taxi_df.pickupHour <= 10), \"AMRush\")\\\n",
        "                                                .when((taxi_df.pickupHour >= 11) & (taxi_df.pickupHour <= 15), \"Afternoon\")\\\n",
        "                                                .when((taxi_df.pickupHour >= 16) & (taxi_df.pickupHour <= 19), \"PMRush\")\\\n",
        "                                                .otherwise(0).alias('trafficTimeBins')\n",
        "                                              )\\\n",
        "                                       .filter((taxi_df.tripTimeSecs >= 30) & (taxi_df.tripTimeSecs <= 7200))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding\n",
        "\n",
        "Different ML algorithms support different types of input, for this example Logistic Regression is being used for Binary Classification. This means that any Categorical (string) variables must be converted to numbers.\n",
        "\n",
        "The process is not as simple as a \"map\" style function as the relationship between the numbers can introduce a bias in the resulting model, the approach is to index the variable and then encode using a std approach called One Hot Encoding.\n",
        "\n",
        "This approach requires the encoder to \"learn\"/fit a model over the data in the Spark instance and then transform based on what was learnt.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {},
      "source": [
        "# The sample uses an algorithm that only works with numeric features convert them so they can be consumed\n",
        "sI1 = StringIndexer(inputCol=\"trafficTimeBins\", outputCol=\"trafficTimeBinsIndex\"); \n",
        "en1 = OneHotEncoder(dropLast=False, inputCol=\"trafficTimeBinsIndex\", outputCol=\"trafficTimeBinsVec\");\n",
        "sI2 = StringIndexer(inputCol=\"weekdayString\", outputCol=\"weekdayIndex\"); \n",
        "en2 = OneHotEncoder(dropLast=False, inputCol=\"weekdayIndex\", outputCol=\"weekdayVec\");\n",
        "\n",
        "# Create a new dataframe that has had the encodings applied\n",
        "encoded_final_df = Pipeline(stages=[sI1, en1, sI2, en2]).fit(taxi_featurised_df).transform(taxi_featurised_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generation of Testing and Training Data Sets\n",
        "Simple split, 70% for training and 30% for testing the model. Playing with this ratio may result in different models.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Decide on the split between training and testing data from the dataframe \n",
        "trainingFraction = 0.7\n",
        "testingFraction = (1-trainingFraction)\n",
        "seed = 1234\n",
        "\n",
        "# Split the dataframe into test and training dataframes\n",
        "train_data_df, test_data_df = encoded_final_df.randomSplit([trainingFraction, testingFraction], seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the Model\n",
        "\n",
        "Train the Logistic Regression model and then evaluate it using Area under ROC as the metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {},
      "source": [
        "## Create a new LR object for the model\n",
        "logReg = LogisticRegression(maxIter=10, regParam=0.3, labelCol = 'tipped')\n",
        "\n",
        "## The formula for the model\n",
        "classFormula = RFormula(formula=\"tipped ~ pickupHour + weekdayVec + passenger_count + tripTimeSecs + trip_distance + fare_amount + payment_type+ trafficTimeBinsVec\")\n",
        "\n",
        "## Undertake training and create an LR model\n",
        "lrModel = Pipeline(stages=[classFormula, logReg]).fit(train_data_df)\n",
        "\n",
        "## Saving the model is optional but its another for of inter session cache\n",
        "datestamp = datetime.now().strftime('%m-%d-%Y-%s');\n",
        "fileName = \"lrModel_\" + datestamp;\n",
        "logRegDirfilename = fileName;\n",
        "lrModel.save(logRegDirfilename)\n",
        "\n",
        "## Predict tip 1/0 (yes/no) on the test dataset, evaluation using AUROC\n",
        "predictions = lrModel.transform(test_data_df)\n",
        "predictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\n",
        "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
        "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate and Visualize\n",
        "\n",
        "Plot the actual curve to develop a better understanding of the model.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {},
      "source": [
        "## Plot the ROC curve, no need for pandas as this uses the modelSummary object\n",
        "modelSummary = lrModel.stages[-1].summary\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'r--')\n",
        "plt.plot(modelSummary.roc.select('FPR').collect(),\n",
        "         modelSummary.roc.select('TPR').collect())\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ]
    }
  ]
}